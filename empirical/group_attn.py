# -*- coding: utf-8 -*-
"""group_focused_attention_events.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1krgbSFvvxaahQtzA1yJnzP6HW2XEcPCq
"""

import pickle
import torch
import numpy as np
import math
import os
import io
from itertools import combinations
import time
from util import read_pickle, apply_transform, get_time_slice, preprocess

"""
WALKTHROUGH:

The data is preprocessed to the following format:
{
  frame_number (int) : {
    tracker_id (int) : [T, J, 3]
  }
}

HOW IT WORKS:
1. sliding window across all frames
2. at any given frame if two individuals are focused on the same spot, increment window size
3. if window breaks:
  a. window is exceeds min threshold, qualifies as group focus -> add seconds
  b. reset window size

you can EDIT the minimum number of frames that it takes to qualify as group focus, and the distance between vectors BELOW
"""

def compute_gaze_vector(joints):
    """
    Computes the gaze direction vector for a single person's joint data.

    Expects joints to be a numpy array of shape [J, 3] where the joints at indices
    57 (left eye), 56 (right eye), and 12 (neck) are used.

    Returns:
        (origin, gaze_vector): A tuple where `origin` is the midpoint between the eyes,
                               and `gaze_vector` is the corrected, normalized gaze direction.
    """
    # Joints and Init 
    L = joints[57]
    R = joints[56]
    N = joints[12]
    mu = (L + R) / 2  # Midpoint between eyes
    initial_gaze = mu - N

    # Define a plane using the vector between the eyes and the initial gaze
    eye_vector = L - R
    plane_normal = np.cross(eye_vector, initial_gaze)
    plane_normal = plane_normal / np.linalg.norm(plane_normal)

    # Projection of the initial gaze onto the plane
    corrected_gaze = initial_gaze - np.dot(initial_gaze, plane_normal) * plane_normal
    corrected_gaze = corrected_gaze / np.linalg.norm(corrected_gaze)
    return mu, corrected_gaze

"""
3 frames => 1 second
MIN TIME SLICE: [5s] 15s 30s

sliding window across all frames
track window size
at any given frame if two individuals are focused on the same spot, increment window size
if window breaks:
  1. window is exceeds min threshold, qualifies as group focus -> add seconds
  2. reset window size

return array of six [0, 0, 0, 0, 0, 0] where each index in the total time focused in that phase

metric should include percentage of time in each phase of focus
"""

def group_focused_attention(data, min_frames, distance_between_vectors_threshold):
  """
  parameters:
  - data: array of dictionaries tracker_id : joints3d

  notes: pad the time for easier window time slice
  """
  group_focus_time = 0

  # sliding window
  l = 0
  r = 0
  event_record = [0 for _ in range(len(data))]
  unique_source_record = [0 for _ in range(len(data))]

  for i, tracker_data in enumerate(data):
    focused_status, num_unique_sources = is_group_focused(tracker_data, distance_between_vectors_threshold)
    unique_source_record[i] = num_unique_sources
    if focused_status:
        r = i
    else:
        if (r - l >= min_frames):
            group_focus_time += (r - l) 
            for frame_idx in range(l, r):
                event_record[frame_idx] += 1
        l = i
        r = i

  #Transform unique source record to account for focus lapses 
  for i in range(len(unique_source_record)):
    unique_source_record[i] = event_record[i] * unique_source_record[i]

  return group_focus_time, event_record, unique_source_record


def is_group_focused(trackers, distance_between_vectors_threshold):
    gaze_vectors = []
    for tracker_id, tracker_data in trackers.items():
        midpoint, gaze_vector = compute_gaze_vector(tracker_data)
        gaze_vectors.append([tracker_id, midpoint, gaze_vector])
    min_distance = float('inf')
    epsilon = 1e-6
    matched_trackers = [] 

    for (t1, O1, v1), (t2, O2, v2) in combinations(gaze_vectors, 2):
        v1 = v1 / np.linalg.norm(v1)
        v2 = v2 / np.linalg.norm(v2)
        # Vector between the two origins
        delta_O = O2 - O1

        # Cross product of direction vectors
        v1_cross_v2 = np.cross(v1, v2)
        norm_v1_cross_v2 = np.linalg.norm(v1_cross_v2)

        if norm_v1_cross_v2 < epsilon:
            # Parallel case: Compute perpendicular distance
            perp_distance = np.linalg.norm(delta_O - np.dot(delta_O, v1) * v1)
        else:
            # Compute the minimum distance using the formula for skew lines
            perp_distance = np.abs(np.dot(delta_O, v1_cross_v2)) / norm_v1_cross_v2
        if perp_distance <= distance_between_vectors_threshold:
            matched_trackers.append((t1, t2))
        min_distance = min(min_distance, perp_distance)
    
    #Count uniqu sources of attention 
    tracker_set = set() 
    num_unique_sources = 0 
    for tk1, tk2 in matched_trackers:
        if tk1 not in tracker_set and tk2 not in tracker_set:
            num_unique_sources += 1    
        tracker_set.add(tk1)
        tracker_set.add(tk2)

    return (min_distance <= distance_between_vectors_threshold, num_unique_sources)


def tester():
    frames = ['frame_000000.pkl', 'frame_009790.pkl']
    time_slice = get_time_slice(frames[0], frames[1], "/content/drive/MyDrive/research/joint_out/", 10)
    time_slice = preprocess(time_slice)
    data = []

    for frame in time_slice:
        tracker_dict = {}
        for i, tracker_id in enumerate(frame['trackers']):
            person_joints = frame['joints3d'][i]
            tracker_dict[tracker_id] = person_joints
        data.append(tracker_dict)

    print("Data processed")
    time_start = time.time()
    group_focused_time = group_focused_attention(data)
    time_end = time.time()
    print(f"Execution time: {time_end - time_start} seconds")
    print(group_focused_time)

def pipeline_tester():
    folder_path = "joint_out/"
    time_slice = get_time_slice(0, 0, folder_path, debug=True)
    time_slice = preprocess(time_slice)
    data = [] 

    # Hyperparameters
    FPS = 3
    MIN_FRAMES = 15
    DISTANCE_BETWEEN_VECTORS_THRESHOLD = 3 

    for frame in time_slice:
        tracker_dict = {}
        for i, tracker_id in enumerate(frame['trackers']):
            person_joints = frame['joints3d'][i]
            tracker_dict[tracker_id] = person_joints
        data.append(tracker_dict)

    print("Data processed")
    time_start = time.time()
    group_focused_time, event_record, unique_source_record = group_focused_attention(data, MIN_FRAMES, DISTANCE_BETWEEN_VECTORS_THRESHOLD)
    time_end = time.time()
    print(f"Execution time: {time_end - time_start} seconds")
    print(len(unique_source_record))
    print(len(event_record))
    print(len(data)) 
    print(group_focused_time)
    breakpoint() 

if __name__ == "__main__":
    pipeline_tester()
    

